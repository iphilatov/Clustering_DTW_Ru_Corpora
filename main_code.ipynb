{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains the code that was used in the research to calculate DTW, perform and plot clustering, see metrics.\n",
        "\n",
        "The dataset used here may be found [on Google Drive](https://drive.google.com/drive/folders/10-Blxv1K0X57u85mXJTX5R1NuRB1qWct?usp=sharing).\n",
        "The code largely depends on the data but one may use basic fucntions and approaches from this notebook to analyse their own data.\n",
        "\n",
        "NB! Due to Google Colab resource restrictions and/or imcompatibily of some libraries, once we received any however big matrix, we could work with it only once before the kernel crashed. Be careful choosing how to run this code and try to save as much data locally/on colab's drive as you can so when a crash happens, your data isn't lost.\n",
        "\n",
        "Contact me if you have any problems running or understanfing the code (tg: iphilatov)"
      ],
      "metadata": {
        "id": "a6CfBQXo76hX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zolraP5xNC4A"
      },
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5Od6eRDieET"
      },
      "outputs": [],
      "source": [
        "!pip install librosa==0.9.2\n",
        "!pip install scikit-bio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPbwb_X1NC4D"
      },
      "outputs": [],
      "source": [
        "from matplotlib.patches import ConnectionPatch\n",
        "from matplotlib.pyplot import figure\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib import cm\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import scipy.spatial.distance as dist\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, median, maxdists, fcluster\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
        "from scipy.spatial.distance import squareform\n",
        "from sklearn.metrics.pairwise import haversine_distances\n",
        "from scipy.stats import pearsonr\n",
        "from skbio.stats.distance import mantel\n",
        "from scipy.io import wavfile\n",
        "from math import radians\n",
        "import IPython.display as ipyd\n",
        "import librosa\n",
        "import os\n",
        "import librosa.display\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tES_YN0sAnm2"
      },
      "outputs": [],
      "source": [
        "# this piece of code is needed when you use data from google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions for DTW"
      ],
      "metadata": {
        "id": "6OO5p5vUo-YE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7idqzGjNC4G"
      },
      "outputs": [],
      "source": [
        "def dp(dist_mat):\n",
        "\n",
        "    N, M = dist_mat.shape\n",
        "    \n",
        "    # Initialize the cost matrix\n",
        "    cost_mat = np.zeros((N + 1, M + 1))\n",
        "    for i in range(1, N + 1):\n",
        "        cost_mat[i, 0] = np.inf\n",
        "    for i in range(1, M + 1):\n",
        "        cost_mat[0, i] = np.inf\n",
        "\n",
        "    # Fill the cost matrix while keeping traceback information\n",
        "    traceback_mat = np.zeros((N, M))\n",
        "    for i in range(N):\n",
        "        for j in range(M):\n",
        "            penalty = [\n",
        "                cost_mat[i, j],      # match (0)\n",
        "                cost_mat[i, j + 1],  # insertion (1)\n",
        "                cost_mat[i + 1, j]]  # deletion (2)\n",
        "            i_penalty = np.argmin(penalty)\n",
        "            cost_mat[i + 1, j + 1] = dist_mat[i, j] + penalty[i_penalty]\n",
        "            traceback_mat[i, j] = i_penalty\n",
        "\n",
        "    # Traceback from bottom right\n",
        "    i = N - 1\n",
        "    j = M - 1\n",
        "    path = [(i, j)]\n",
        "    while i > 0 or j > 0:\n",
        "        tb_type = traceback_mat[i, j]\n",
        "        if tb_type == 0:\n",
        "            # Match\n",
        "            i = i - 1\n",
        "            j = j - 1\n",
        "        elif tb_type == 1:\n",
        "            # Insertion\n",
        "            i = i - 1\n",
        "        elif tb_type == 2:\n",
        "            # Deletion\n",
        "            j = j - 1\n",
        "        path.append((i, j))\n",
        "\n",
        "    # Strip infinity edges from cost_mat before returning\n",
        "    cost_mat = cost_mat[1:, 1:]\n",
        "    return (path[::-1], cost_mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wkx_AIfaOq7"
      },
      "outputs": [],
      "source": [
        "# this code compares two audio files by DTW\n",
        "\n",
        "def two_comparison(a_audio, b_audio):\n",
        "  f_sx, x = wavfile.read(a_audio)\n",
        "  f_sy, y = wavfile.read(b_audio)\n",
        "\n",
        "  n_fft = int(0.025*f_s)      # 25 ms\n",
        "  hop_length = int(0.01*f_s)  # 10 ms\n",
        "  \n",
        "  mel_spec_x = librosa.feature.melspectrogram(\n",
        "    x/1.0, sr=f_sx, n_mels=40,\n",
        "    n_fft=n_fft, hop_length=hop_length\n",
        "    )\n",
        "  log_mel_spec_x = np.log(mel_spec_x)\n",
        "\n",
        "  mel_spec_y = librosa.feature.melspectrogram(\n",
        "    y/1.0, sr=f_sy, n_mels=40,\n",
        "    n_fft=n_fft, hop_length=hop_length\n",
        "    )\n",
        "  log_mel_spec_y = np.log(mel_spec_y)\n",
        "\n",
        "  x_seq = log_mel_spec_x.T\n",
        "  y_seq = log_mel_spec_y.T\n",
        "\n",
        "  dist_mat = dist.cdist(x_seq, y_seq, \"cosine\")\n",
        "  path, cost_mat = dp(dist_mat)\n",
        "  print(\"Alignment cost: {:.4f}\".format(cost_mat[-1, -1]))\n",
        "\n",
        "  M = y_seq.shape[0]\n",
        "  N = x_seq.shape[0]\n",
        "  print(\n",
        "        \"Normalized alignment cost: {:.8f}\".format(\n",
        "        cost_mat[-1, -1]/(M + N))\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJ1lYNbZc6VR"
      },
      "outputs": [],
      "source": [
        "# this code compares multiple files by DTW\n",
        "\n",
        "def comparison(list_of_audio):\n",
        "  \n",
        "  q = len(list_of_audio)\n",
        "  seq_dict = dict()\n",
        "  not_df = dict()\n",
        "\n",
        "  for audio in list_of_audio:\n",
        "    \n",
        "    audio_name = str(audio)\n",
        "\n",
        "    f_s, z = wavfile.read(audio)\n",
        "    \n",
        "    n_fft = int(0.025*f_s)      # 25 ms\n",
        "    hop_length = int(0.01*f_s)  # 10 ms\n",
        "\n",
        "    mel_spec_z = librosa.feature.melspectrogram(\n",
        "    z/1.0, sr=f_s, n_mels=40,\n",
        "    n_fft=n_fft, hop_length=hop_length\n",
        "    )\n",
        "  \n",
        "    log_mel_spec_z = np.log(mel_spec_z)\n",
        "\n",
        "    z_seq = log_mel_spec_z.T\n",
        "\n",
        "    seq_dict[audio_name] = z_seq\n",
        "\n",
        "  for name_main, seq_main in seq_dict.items():\n",
        "    \n",
        "    row_dict = dict()\n",
        "\n",
        "    M = seq_main.shape[0]\n",
        "\n",
        "    for name_snd, seq_snd in seq_dict.items():\n",
        "\n",
        "      dist_mat = dist.cdist(seq_main, seq_snd, \"cosine\")\n",
        "      path, cost_mat = dp(dist_mat)\n",
        "\n",
        "      N = seq_snd.shape[0]\n",
        "\n",
        "      normalized = cost_mat[-1, -1]/(M + N)\n",
        "\n",
        "      row_dict[name_snd] =  \"%.8f\" % normalized\n",
        "    \n",
        "    not_df[name_main] = row_dict\n",
        "\n",
        "  return pd.DataFrame(not_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRkY39JIYWPr"
      },
      "outputs": [],
      "source": [
        "# this function may be handy if one wants to see isc visualization of an audio\n",
        "def osc_visual(audio_file):\n",
        "  y, sr = librosa.load(audio_file)\n",
        "  librosa.display.waveshow(y, sr=sr) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAfhIMekOTo1"
      },
      "source": [
        "# Original clustering\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXviNU02n6gH"
      },
      "outputs": [],
      "source": [
        "# this is the most basic clustering and its visualisations\n",
        "#start here if you just need some cluster analysis and a dendrogram\n",
        "\n",
        "def clustering_alpha(df, draw = True, fs = (20,15)):\n",
        "\n",
        "  # make an condensed matrix from an uncondensed one \n",
        "  condense = squareform(df, checks=False)\n",
        "  # you may want to try this line instead of the one above if it doesn't work\n",
        "  #condense = squareform(df) \n",
        "\n",
        "  # getting labels for the plot (not the best way to use pandas though)\n",
        "  labels = list(df.columns)\n",
        "\n",
        "  # calculating clusters here (needs lots of resource)\n",
        "  H = linkage(condense, 'ward')\n",
        "\n",
        "  # make a dendrogram\n",
        "  if draw == True:\n",
        "    fig = plt.figure(figsize=fs)\n",
        "    dn = dendrogram(H, labels = labels, orientation='right')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJHfA3MPLSvq"
      },
      "source": [
        "# Creating data to use for clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "gxaGJlptqpCM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mSoIBmELSvq"
      },
      "outputs": [],
      "source": [
        "# reading names of files here  \n",
        "\n",
        "first_audio_names = [file for file in os.listdir('/content/drive/MyDrive/output')]\n",
        "len(first_audio_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ex-Rd8HBLSvr"
      },
      "outputs": [],
      "source": [
        "# here we check that all audio fit our name pattern; if some don't, we'll work with them later\n",
        "\n",
        "count = 0\n",
        "\n",
        "for file in first_audio_names:\n",
        "    if re.fullmatch(\"\\d+_(bylo|eto|menja|oni|tozhe)_[A-Za-z]+\\d*_\\w+.wav\", file):\n",
        "        count += 1\n",
        "    else:\n",
        "        print(file) \n",
        "\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY1H71uNPyVb"
      },
      "outputs": [],
      "source": [
        "# getting rid of extension info in file names\n",
        "\n",
        "all_audio_names = list()\n",
        "\n",
        "for name in first_audio_names:\n",
        "  if name[-4:] == '.wav':\n",
        "    name = name[:-4]    \n",
        "  all_audio_names.append(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAPW0tDTBpeg"
      },
      "outputs": [],
      "source": [
        "# this function lets you retrive certain subsets from the dataset; you can pick files by a speaker, a word and a corpus \n",
        "\n",
        "def corpora_search(data_list, word = None, speaker = None, corpus = None):\n",
        "\n",
        "  if word in ['bylo', 'eto', 'oni', 'tozhe']:\n",
        "    by_word_list = [name for name in data_list if re.fullmatch(\"\\d+_{}_[A-Za-z]+\\d*_\\w+\".format(word), name)]\n",
        "  # \"меня\" has some exceptions so you nay want to process them in a different way\n",
        "  elif word == 'menja':\n",
        "    by_word_list  = [name for name in data_list if re.fullmatch(\"\\d+_menja_([A-Za-z]+@)?[A-Za-z]+\\d*_\\w+\", name)]\n",
        "  elif word == None:\n",
        "    by_word_list = data_list\n",
        "  else:\n",
        "    print('Error: wrong word choice')\n",
        "\n",
        "  if speaker == None:\n",
        "    by_speaker_list = by_word_list\n",
        "  else:\n",
        "    by_speaker_list = [name for name in by_word_list if re.fullmatch(\"\\d+_[A-Za-z]+_{}_\\w+\".format(speaker), name)]\n",
        "\n",
        "  if corpus == None:\n",
        "    by_corpus_list = by_speaker_list\n",
        "  else:\n",
        "    by_corpus_list = [name for name in by_speaker_list if re.fullmatch(\"\\d+_[A-Za-z]+_([A-Za-z]+@)?[A-Za-z]+\\d*_{}\".format(corpus), name)]\n",
        "\n",
        "  return by_corpus_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByiMPtLSOXvV"
      },
      "outputs": [],
      "source": [
        "# creating a subset of data which contains examples of the pronunciation of the word 'тоже'\n",
        "\n",
        "the_word = 'tozhe'\n",
        "only_one_word = corpora_search(all_audio_names, word = the_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81HRp4UP55Nz"
      },
      "source": [
        "## Binding meta data from corpus with the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKi7tjxBQwb6"
      },
      "outputs": [],
      "source": [
        "# giving each corpus its own colour; we practically do not need a file with meta data here\n",
        "\n",
        "list_of_corpora = list() \n",
        "for name in all_audio_names:\n",
        "  corpora_name = name.split('_')[-1].split('.')[0]\n",
        "  if corpora_name not in list_of_corpora and corpora_name[-1] != ')':\n",
        "    list_of_corpora.append(corpora_name)\n",
        "\n",
        "colours = ['black', 'green', 'red', 'cyan', 'crimson', 'yellow', 'blue', 'rosybrown', 'magenta', 'brown', 'olive', 'teal', 'orangered', 'purple', 'indigo']\n",
        "len(colours)\n",
        "\n",
        "color4cor = dict(zip(list_of_corpora, colours))\n",
        "\n",
        "color_by_corpus = dict()\n",
        "\n",
        "for name in all_audio_names:\n",
        "  place = name.split('_')[-1].split('.')[0]\n",
        "  colour = color4cor[place]\n",
        "  color_by_corpus[name] = colour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIYMPKpS6KD0"
      },
      "outputs": [],
      "source": [
        "# reading meta data\n",
        "\n",
        "xlsx_path = '/content/drive/MyDrive/meta.xlsx'\n",
        "meta_df = pd.read_excel(xlsx_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e79YLRds55N3"
      },
      "outputs": [],
      "source": [
        "# giving each gender its colour - we have only males and females, so the colours user are blue (males) and red (females)\n",
        "\n",
        "color_by_gender = dict()\n",
        "\n",
        "for name in all_audio_names:\n",
        "    \n",
        "    person = name.split('_')[2]\n",
        "\n",
        "    try:\n",
        "        sex = meta_df.loc[meta_df.string_id == person].gender.item()\n",
        "    except ValueError:\n",
        "        continue\n",
        "    \n",
        "    if sex.lower() == 'f':\n",
        "        color = 'red'\n",
        "    if sex.lower() == 'm':\n",
        "        color = 'blue'\n",
        "\n",
        "    color_by_gender[name] = color\n",
        "\n",
        "# working with exceptions \n",
        "for name in all_audio_names:\n",
        "    if name.split('_')[2] == 'MNS1916' or name.split('_')[2] == 'tx@MLI1941':\n",
        "        color_by_gender[name] = 'red'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lv7LNM7O55N4"
      },
      "outputs": [],
      "source": [
        "# creating a colour grade and giving each file its colour on it based on speaker's age\n",
        "\n",
        "color_by_age = dict()\n",
        "cmap = cm.get_cmap('viridis')\n",
        "\n",
        "for name in all_audio_names:\n",
        "    \n",
        "    person = name.split('_')[2]\n",
        "\n",
        "    # the research was made in 2023\n",
        "    try:\n",
        "        age = 2023 - int(meta_df.loc[meta_df.string_id == person].year_of_birth.item())\n",
        "    except ValueError:\n",
        "        continue\n",
        "\n",
        "    color = cmap(1-age/100)\n",
        "\n",
        "    color_by_age[name] = color\n",
        "\n",
        "# working with exceptions \n",
        "for name in all_audio_names:\n",
        "    if name.split('_')[2] == 'MNS1916':\n",
        "        color_by_age[name] = cmap((2023-1916)/100)\n",
        "    elif name.split('_')[2] == 'tx@MLI1941':\n",
        "        color_by_age[name] = cmap((2023-1941)/100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this code creates a dictionary where each file is assigned to its corpus; may be useful later\n",
        "\n",
        "file_corpora_dict = dict()\n",
        "\n",
        "for audio in all_audio_names:\n",
        "  for corpora in list_of_corpora:\n",
        "    if corpora in audio:\n",
        "      file_corpora_dict[audio] = corpora"
      ],
      "metadata": {
        "id": "fl5SYKzh9lPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET2hDzgc55Nx"
      },
      "source": [
        "## Calculating a matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Sbh-gZ0CEiU"
      },
      "outputs": [],
      "source": [
        "# this is a modified version of `comparison` function which works better with research's dataset\n",
        "\n",
        "def comparison4output(list_of_audio):\n",
        "\n",
        "  q = len(list_of_audio)\n",
        "  seq_dict = dict()\n",
        "  not_df = dict()\n",
        "\n",
        "  for audio in list_of_audio:\n",
        "    \n",
        "    audio_path = '/content/drive/MyDrive/output/' + audio + '.wav'\n",
        "\n",
        "    audio_name = str(audio)\n",
        "\n",
        "    f_s, z = wavfile.read(audio_path)\n",
        "    \n",
        "    n_fft = int(0.025*f_s)      # 25 ms\n",
        "    hop_length = int(0.01*f_s)  # 10 ms\n",
        "\n",
        "    mel_spec_z = librosa.feature.melspectrogram(\n",
        "    z/1.0, sr=f_s, n_mels=40,\n",
        "    n_fft=n_fft, hop_length=hop_length\n",
        "    )\n",
        "  \n",
        "    log_mel_spec_z = np.log(mel_spec_z)\n",
        "\n",
        "    z_seq = log_mel_spec_z.T\n",
        "\n",
        "    seq_dict[audio_name] = z_seq\n",
        "\n",
        "  for name_main, seq_main in seq_dict.items():\n",
        "    \n",
        "    row_dict = dict()\n",
        "\n",
        "    M = seq_main.shape[0]\n",
        "\n",
        "    for name_snd, seq_snd in seq_dict.items():\n",
        "\n",
        "      dist_mat = dist.cdist(seq_main, seq_snd, \"cosine\")\n",
        "      path, cost_mat = dp(dist_mat)\n",
        "\n",
        "      N = seq_snd.shape[0]\n",
        "\n",
        "      normalized = cost_mat[-1, -1]/(M + N)\n",
        "\n",
        "      row_dict[name_snd] =  \"%.8f\" % normalized\n",
        "    \n",
        "    not_df[name_main] = row_dict\n",
        "\n",
        "  return pd.DataFrame(not_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5RakZMCLSvr"
      },
      "outputs": [],
      "source": [
        "# calculating a matrix\n",
        "# calculation may take pretty long\n",
        "\n",
        "matrix = comparison4output(only_one_word)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# it is recommended to save the matrix as Colab is bad at handling lots of massive so a kernel may crash\n",
        "matrix.to_pickle('orig.pkl')"
      ],
      "metadata": {
        "id": "35o0MKL5QdFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Many clusterings\n",
        "This section present a couple of approaches to work with clustering that were used in our research "
      ],
      "metadata": {
        "id": "uzBu8TzLqu8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering with meta data"
      ],
      "metadata": {
        "id": "5SLG9CYorUiY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXlvTg1mJCC1"
      },
      "outputs": [],
      "source": [
        "# this function is based on the original clustering function\n",
        "# it uses meta data to colour labels \n",
        "\n",
        "def clustering_beta(df, coloring, draw=True, fs=(20, 15)):\n",
        "    labels = [str(label) for label in df.columns]\n",
        "    condense = squareform(df, checks=False)\n",
        "    H = linkage(condense, 'ward')\n",
        "\n",
        "    # creating the dendrogram and set the color of the leaves\n",
        "    if draw:\n",
        "        fig = plt.figure(figsize=fs)\n",
        "        dn = dendrogram(H, labels=labels, orientation='left', color_threshold = 0)\n",
        "\n",
        "        ax = plt.gca()\n",
        "        ax.invert_yaxis()\n",
        "        xlbls = ax.get_ymajorticklabels()\n",
        "        for lbl in xlbls:\n",
        "            color_decider = lbl.get_text()\n",
        "            lbl.set_color(coloring[color_decider])\n",
        "\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d2eOsl9dtC7"
      },
      "outputs": [],
      "source": [
        "clustering_beta(matrix, color_by_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering and plotting independently"
      ],
      "metadata": {
        "id": "ZT_AHgNB08O8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# these two functions do basically the same as the previous one but they are separated\n",
        "# so labels and linkage matrix don't have to calculated again to plot a dendrogram with different meta dat\n",
        "\n",
        "def clustering_delta(df):\n",
        "  condense = squareform(df, checks=False)\n",
        "  labels = list(df.columns)\n",
        "  H = linkage(condense, 'ward')\n",
        "  return H, labels\n",
        "\n",
        "def drawing_clustering_delta(clustered_matrix, labels, coloring, fs=(20, 15)):\n",
        "  fig = plt.figure(figsize=fs)\n",
        "  dn = dendrogram(clustered_matrix, labels=labels, orientation='left', color_threshold = 0)\n",
        "\n",
        "  ax = plt.gca()\n",
        "  ax.invert_yaxis()\n",
        "  xlbls = ax.get_ymajorticklabels()\n",
        "  for lbl in xlbls:\n",
        "    color_decider = lbl.get_text()\n",
        "    lbl.set_color(coloring[color_decider])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "xgPD9DyA0-G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clustered, labels = clustering_delta(matrix)"
      ],
      "metadata": {
        "id": "TROdcIQX1MLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drawing_clustering_delta(clustered, labels, color_by_age, fs=(20, 40))"
      ],
      "metadata": {
        "id": "-zXw0l1zrO_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics"
      ],
      "metadata": {
        "id": "THg7QXkSq3i8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ARI & NMI"
      ],
      "metadata": {
        "id": "ZyMv9-1Kp6PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we need only corpora/villages that are present in the subset\n",
        "\n",
        "data_dict = dict()\n",
        "villages_list = list()\n",
        "\n",
        "for audio, village in file_corpora_dict.items():\n",
        "  if audio in only_one_word:\n",
        "    data_dict[audio] = village\n",
        "    if village not in villages_list:\n",
        "      villages_list.append(village)\n",
        "\n",
        "data = list(data_dict.keys())\n",
        "true_labels = list(data_dict.values())\n",
        "\n",
        "# choosing a threshold for the linkage distance based on the number of clusters in the ground truth\n",
        "t = len(villages_list)\n",
        "\n",
        "# cutting a cluster tree - retrieving flat clustering and labels\n",
        "labels = fcluster(clustered, t, criterion='maxclust')\n",
        "\n",
        "if len(true_labels) != len(labels):\n",
        "    raise ValueError('The number of samples in true_labels and labels are different')\n",
        "\n",
        "# calculating the metrics between the true labels and the clustering labels\n",
        "nmi = normalized_mutual_info_score(true_labels, labels)\n",
        "ari = adjusted_rand_score(true_labels, labels)\n",
        "\n",
        "print('These are metrics for the word \"{}\".'.format(the_word))\n",
        "print(f\"The NMI score is {nmi:.8f}\")\n",
        "print(f\"The ARI score is {ari:.8f}\")"
      ],
      "metadata": {
        "id": "XYqpsSnssP0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"{}_nmi.txt\".format(the_word), \"w\") as nmi_file:\n",
        "    nmi_file.write(str(nmi))\n",
        "\n",
        "with open(\"{}_ari.txt\".format(the_word), \"w\") as ari_file:\n",
        "    ari_file.write(str(ari))"
      ],
      "metadata": {
        "id": "Cxkds04X0Nxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing geographical data with audio corpus data\n",
        "\n"
      ],
      "metadata": {
        "id": "O6m8w4T34i5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# that is where we upload a saved matrix from `comparison` function bact to the environment \n",
        "\n",
        "matrix = pd.read_pickle(\"orig.pkl\")"
      ],
      "metadata": {
        "id": "tO8ojevuS1NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieving coordinates \n",
        "\n",
        "tsv_path = '/content/drive/MyDrive/corpora_villages.tsv'\n",
        "geo_df = pd.read_table(tsv_path)\n",
        "only_needed_geo_info = geo_df[['name', 'lat', 'lon']]\n",
        "only_needed_geo_info = only_needed_geo_info.drop(labels=[97], axis=0)"
      ],
      "metadata": {
        "id": "ka46VS2UEbEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_df = []\n",
        "geo_df = []"
      ],
      "metadata": {
        "id": "T4OLEBtoP6J-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this dictionary contains corpora's names in the dataset as keys and the names from geo.dataframe as values\n",
        "\n",
        "dict_df_village_names = {'rogovatka':'Corpus of Rogovatka dialect',\n",
        " 'malinino':'Corpus of the Russian dialect spoken in the village Malinino',\n",
        " 'manturovo':'Corpus of the Russian dialect spoken in Manturovo',\n",
        " 'lukhteza':'Corpus of Lukh and Teza river basins dialects',\n",
        " 'teza':'Corpus of Lukh and Teza river basins dialects',\n",
        " 'pyoza':'Corpus of the Russian dialect spoken in the villages of the Middle Pyoza',\n",
        " 'luzhnikovo':'Luzhnikovo Corpus',\n",
        " 'tserkovnoe':'Corpus of the Russian dialect spoken in Tserkovnoe',\n",
        " 'don':'Corpus of the Russian dialect spoken in the villages of the Don river',\n",
        " 'khislavichi':'Corpus of the Russian dialect spoken in Khislavichi district',\n",
        " 'makeevo':'Corpus of Shetnevo and Makeevo',\n",
        " 'nekhochi':'Corpus of the Russian dialect spoken in Nekhochi',\n",
        " 'vyya':'Upper Pinega and Vyya Corpus',\n",
        " 'keba':'Corpus of the Russian dialect spoken in Keba'}"
      ],
      "metadata": {
        "id": "g4Q-A6s-dfdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# thus fucntions calculates medians for each corpus' audio\n",
        "\n",
        "def calculate_medians(matrix):\n",
        "  df_for_median = matrix.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "  class_names = list(set([x.split('_')[-1] for x in df_for_median.index]))\n",
        "  class_distances = {}\n",
        "\n",
        "  for class_name in class_names:\n",
        "      class_rows = [x for x in df_for_median.index if x.split('_')[-1] == class_name]\n",
        "      class_distances[class_name] = df_for_median.loc[class_rows, class_rows].median().median()\n",
        "\n",
        "  new_matrix = pd.DataFrame(columns=class_names, index=class_names)\n",
        "\n",
        "  for i in range(len(class_names)):\n",
        "      for j in range(len(class_names)):\n",
        "          if i == j:\n",
        "              new_matrix.iloc[i, j] = 0\n",
        "          else:\n",
        "              new_matrix.iloc[i, j] = class_distances[class_names[i]] + class_distances[class_names[j]]\n",
        "\n",
        "  new_matrix -= df_for_median.median().median()\n",
        "\n",
        "  new_matrix.values[[np.arange(len(class_names))]*2] = 0\n",
        "\n",
        "  return class_names, new_matrix"
      ],
      "metadata": {
        "id": "9jFyHzyUenjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this function compares the medians with the coordinates and calculates mantel test\n",
        "# NB this function actually broke kernels - we used it just to get each and save it\n",
        "# virtually, we used the code after this function to get the results of mantel test\n",
        "\n",
        "def compare_geo_audio(comparison_matrix, dict_df_village_names = dict_df_village_names, coor_df = only_needed_geo_info):\n",
        "  \n",
        "  villages, village_medians = calculate_medians(comparison_matrix)\n",
        "\n",
        "  list_of_villages_in_matrix = list()\n",
        "\n",
        "  for village in villages:\n",
        "    for file_name, df_name in dict_df_village_names.items():\n",
        "      if village == file_name:\n",
        "        list_of_villages_in_matrix.append(df_name)\n",
        "\n",
        "  village_location_list = list()\n",
        "\n",
        "  for one_village in list_of_villages_in_matrix:\n",
        "  \n",
        "    subdf_1row = coor_df.loc[coor_df.name == one_village]\n",
        "    village_name = subdf_1row.name.iloc[0]\n",
        "\n",
        "    lat = radians(subdf_1row.lat.iloc[0])\n",
        "    lon = radians(subdf_1row.lon.iloc[0])\n",
        "    \n",
        "    village_location_list.append([lat, lon])\n",
        "\n",
        "  geo_distance_matrix = haversine_distances(village_location_list)\n",
        "  geo_distance_matrix = pd.DataFrame(geo_distance_matrix * 6371000/1000)\n",
        "\n",
        "  village_medians_sqr_mtx, geo_sqr_mtx = squareform(village_medians), squareform(geo_distance_matrix) \n",
        "  \n",
        "  mantel_test = mantel(village_medians_sqr_mtx, geo_sqr_mtx)\n",
        "\n",
        "  return mantel_test\n",
        "  #return geo_distance_matrix\n",
        "  #return village_medians"
      ],
      "metadata": {
        "id": "QtFMfz8t6qRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We saved one matrix then restarted the kernel and save another one.\n",
        "Then we restarted the kernel again and used the saved data to perform Mantel test"
      ],
      "metadata": {
        "id": "9Edus8hE7UqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here we saved data\n",
        "\n",
        "#compare_geo_audio(matrix).to_pickle(\"geo.pkl\")\n",
        "compare_geo_audio(matrix).to_pickle(\"vill.pkl\")"
      ],
      "metadata": {
        "id": "QL5XfPxrgC2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here we read data\n",
        "\n",
        "geo_distance = pd.read_pickle(\"geo.pkl\")\n",
        "vill_medians = pd.read_pickle(\"vill.pkl\")"
      ],
      "metadata": {
        "id": "4kcPhEC9ihRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating mantel test\n",
        "\n",
        "mantel_test = mantel(squareform(vill_medians), squareform(geo_distance))"
      ],
      "metadata": {
        "id": "ZNM-1RvCaURe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the result in a file\n",
        "\n",
        "with open(\"{}_mantel.txt\".format('menja'), \"w\") as mantel_file:\n",
        "    mantel_file.write(str(mantel_test))"
      ],
      "metadata": {
        "id": "V30nb1PdThqH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fAfhIMekOTo1",
        "7gpPmHn3HAuA",
        "ZT_AHgNB08O8"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}